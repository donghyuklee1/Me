<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Donghyuk Lee</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <div class="main-content">
            <div class="left-column">
                <h1 class="name">DONGHYUK LEE</h1>
                <div class="section">
                    <h2 class="section-title">Research:</h2>
                    <ul class="list">
                        <li><a href="#" class="menu-item" data-type="research" data-item="1">Light Spectrum Predicting for VR</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="2">DeepMetricEye</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="3">Realistic Earthquake Simulation</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="4">Fungal Morphology Simulation</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="5">Delta-E Color Shift Method</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="6">Semantic Artificial Life System</a></li>
                        <li><a href="#" class="menu-item" data-type="research" data-item="7">Spinal Postural Wearable Devices Review</a></li>
                    </ul>
                </div>

                <div class="footer-links">
                    <a href="#" class="link" id="about-link">About</a>
                    <a href="#" class="link" id="contact-link">Contact</a>
                    <a href="#" class="link" id="photos-link">Photos</a>
                </div>
            </div>
            
            <div class="color-picker">
                <div class="color-dot" id="theme-dot"></div>
            </div>
        
            <div class="theme-toggle" id="theme-toggle">
                <span class="theme-icon" id="theme-icon">üïäÔ∏è</span>
            </div>
            
            <div class="memo-toggle" id="memo-toggle"></div>
            
            <div class="animation-mouse" id="animation-mouse">üêÅ</div>
            
            <div class="memo-section" id="memo-section" style="display: none;">
                <div class="memo-content">
                    <div class="memo-header">
                        <h3 class="memo-title">Status Update</h3>
                        <button class="memo-close" id="memo-close">√ó</button>
                    </div>
                    <div class="memo-body">
                        <div class="memo-display" id="memo-display">
                            <div class="memo-entry">
                                <div class="memo-display-date">01.03.2026</div>
                                <div class="memo-display-message">I move to my new website</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="right-column">
                <!-- About Section -->
                <div class="content-section" id="about-section">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">About</h2>
                        <p class="biography">
                            I am an undergraduate student in the School of Electrical Engineering at KAIST and a member of the artificial intelligence club Include and the KAIST Ski Club (KARVE). I am studying with an interest in research on biomedical devices based on computational methodologies such as machine learning and in the field of signal processing.
                        </p>
                        
                        <div class="about-section-content">
                            <p class="biography">
                                During my time at science high school, I conducted various research activities combining graph theory and genetics. Specifically, I developed a screening model that rapidly predicts diabetes based on the 5hMC methylation pattern of circulating cell-free DNA (cfDNA). Additionally, under the guidance of Professor Jin-Soo Seo (Yonsei university, Seo Lab), I conducted research using graph perturbation theory to elucidate the correlation between the ApoE4 genotype (a causative gene for Alzheimer's disease) and lipid rafts from a metabolic perspective. Beyond this, I have made diverse attempts to integrate various machine learning techniques, such as GANs and LLMs, into biotechnology research.
                            </p>
                            <p class="biography">
                                I also possess wet lab research experience. During my sophomore year of high school, I conducted research in Professor Yoon's lab at KAIST (Yoon Lab) on elucidating cell-specific transcriptomic dynamics during human brain organoid development. During this research, I had the opportunity to use confocal imaging, which sparked my interest in imaging and processing techniques. Consequently, I am currently pursuing personal studies related to optics.
                            </p>
                            <p class="biography">
                                I'm very interested in audio equipments. Whenever new audio gear or codecs are released, I always attend listening sessions. Beyond that, I often drum on a set and make house beats as a hobby. And I absolutely love skiing!
                            </p>
                            
                            <h3 class="about-subtitle">Professional Roles and Projects:</h3>
                            
                            <ul class="about-list">
                                <li class="about-list-item">
                                    <strong>Visiting Scholar, MIT CSAIL</strong> (2025‚ÄìPresent): Dr. Lee is currently a visiting researcher at MIT's Computer Science & Artificial Intelligence Laboratory, where he collaborates with Prof. Stefanie Mueller's group. At CSAIL he develops intelligent interfaces using conductive flexible fabrics and explores bio-inspired computational materials (e.g. simulating fungal growth for self-grown textiles).
                                </li>
                                <li class="about-list-item">
                                    <strong>Editor-in-Chief, interactives Journal</strong> (2025‚ÄìPresent): He is the founding Editor-in-Chief of <em>interactives</em> (ISSN 2755-6336), an experimental open-access journal focused on HCI, game studies, and new media. Dr. Lee leads the journal's editorial direction and ethics, and he spearheaded the development of its digital platform enabling interactive content in research papers (integrating videos, demos, and an AI-assisted review process).
                                </li>
                                <li class="about-list-item">
                                    <strong>Postdoctoral Research, RCA & XR Network+</strong> (2024‚Äì2025): As a Postdoctoral Research Associate at the Royal College of Art, Dr. Lee co-led a UKRI/EPSRC-funded project in partnership with Foster + Partners and the University of New South Wales. He helped develop a high-fidelity earthquake simulation environment in Unreal Engine for urban disaster modeling and virtual production. This interdisciplinary project advanced realistic visualization for disaster training and was recognized as an Outstanding Project by the XR Network+ program.
                                </li>
                            </ul>
                            
                            <h3 class="about-subtitle">Publications and Patents</h3>
                            <p class="biography">
                                Dr. Lee has authored numerous peer-reviewed publications in leading computer graphics and HCI venues. Notably, his recent work on VR display optimization was accepted in the IEEE <em>Transactions on Visualization and Computer Graphics</em> (TVCG), and he has presented research at the ACM SIGGRAPH Asia conference. He is also the inventor of several patent-pending technologies, including a method for decoding pupillary oscillations to infer brain arousal and a color calibration technique for VR displays ‚Äì the latter already licensed for use in commercial VR platforms.
                            </p>
                            
                            <h3 class="about-subtitle">Awards and Honors</h3>
                            <p class="biography">
                                Dr. Lee's work has earned international recognition. He received an <em>Honorary Mention</em> at the Prix Ars Electronica (2018) and a <em>New Face Award</em> from the Japan Media Arts Festival (2019), honors that highlight the creative and impactful nature of his projects. During his early career, one of his interactive designs was even collected by a national art museum, and he won First Prize in his academy's graduation exhibition ‚Äì early achievements that reflect his strong artistic roots. Throughout his career, Dr. Donghyuk Lee has remained dedicated to bridging the worlds of scientific research and interactive technology, integrating his design background with advanced engineering to drive innovation.
                            </p>
                        </div>
                        
                        <div class="photo-container">
                            <div class="photo-placeholder"></div>
                        </div>
                    </div>
                </div>

                <div class="content-section" id="contact-section" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Contact</h2>
                        <div class="contact-content">
                            <p class="contact-item">
                                <strong>Email:</strong> dhlee4832@kaist.ac.kr
                            </p>
                            <p class="contact-item">
                                <strong>Location:</strong> KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon 34141, South Korea
                            </p>
                            <p class="contact-item">
                                <strong>LinkedIn:</strong> <a href="#" class="contact-link">linkedin.com/in/donghyuklee</a>
                            </p>
                            <p class="contact-item">
                                <strong>GitHub:</strong> <a href="#" class="contact-link">github.com/donghyuklee</a>
                            </p>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-1" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Light Spectrum Predicting for VR</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: An overview of our approach.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    Predicting the Light Spectrum of Virtual Reality Scenarios for Non-Image-Forming Visual Evaluation. 
                                    This research presents a multi-stage process for analyzing virtual reality scenarios to evaluate 
                                    their effect on the Non-Image-Forming (NIF) visual system, aiming to reduce digital eye strain 
                                    and sleep disorders.
                                </p>
                                <h3 class="research-subtitle">Methodology</h3>
                                <p class="research-description">
                                    The approach involves extracting optical data from VR headsets, processing virtual scenarios through 
                                    field of view clipping, lens attenuation calculation, gamma correction, and spectral conversion to 
                                    predict melanopic equivalent daylight illuminance (M-EDI) and five photoreceptor radiation efficacy (FPRE) maps.
                                </p>
                                <h3 class="research-subtitle">Publication</h3>
                                <p class="research-description">
                                    Presented at IEEE VR 2023
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-2" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">DeepMetricEye</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: Deep learning-based eye-tracking system architecture.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    DeepMetricEye is a deep learning-based eye-tracking system designed for precise gaze measurement 
                                    and analysis. The system utilizes advanced neural network architectures to achieve high accuracy 
                                    in eye movement tracking and gaze prediction.
                                </p>
                                <h3 class="research-subtitle">Key Features</h3>
                                <p class="research-description">
                                    Real-time gaze tracking, high precision measurement, and robust performance across various 
                                    lighting conditions and user demographics.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Research Section 3: Realistic Earthquake Simulation -->
                <div class="content-section research-detail-section" id="research-item-3" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Realistic Earthquake Simulation</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: Digital twin simulation environment.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    Digital twin simulations for realistic earthquake modeling and disaster preparedness. 
                                    This project develops high-fidelity earthquake simulation environments using Unreal Engine 
                                    for urban disaster modeling and virtual production.
                                </p>
                                <h3 class="research-subtitle">Applications</h3>
                                <p class="research-description">
                                    Disaster training, urban planning, emergency response preparation, and virtual production 
                                    for realistic visualization of seismic events.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-4" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Fungal Morphology Simulation</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: Computational modeling of fungal growth patterns.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    Computational modeling of fungal growth patterns and morphological development. 
                                    This research explores bio-inspired computational materials, simulating fungal growth 
                                    for self-grown textiles and intelligent interfaces.
                                </p>
                                <h3 class="research-subtitle">Research Context</h3>
                                <p class="research-description">
                                    Conducted at MIT CSAIL in collaboration with Prof. Stefanie Mueller's group, 
                                    exploring conductive flexible fabrics and bio-inspired computational materials.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-5" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Delta-E Color Shift Method</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: Color calibration technique for VR displays.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    Novel method for measuring and correcting color shifts in display systems. 
                                    This color calibration technique for VR displays has been licensed for use 
                                    in commercial VR platforms.
                                </p>
                                <h3 class="research-subtitle">Impact</h3>
                                <p class="research-description">
                                    Patent-pending technology that improves color accuracy in virtual reality 
                                    displays, enhancing user experience and visual fidelity.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-6" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Semantic Artificial Life System</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: AI system architecture for artificial life simulation.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    AI system for creating and simulating artificial life forms with semantic understanding. 
                                    This research combines artificial intelligence with biological simulation to create 
                                    intelligent virtual organisms.
                                </p>
                                <h3 class="research-subtitle">Capabilities</h3>
                                <p class="research-description">
                                    Semantic understanding, evolutionary simulation, and interactive artificial life forms 
                                    with adaptive behaviors.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section research-detail-section" id="research-item-7" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Spinal Postural Wearable Devices Review</h2>
                        <div class="research-detail-content">
                            <div class="research-figure-container">
                                <div class="research-figure">
                                    <div class="figure-placeholder"></div>
                                    <p class="figure-caption">Figure 1: Comprehensive review of wearable devices for spinal posture monitoring.</p>
                                </div>
                            </div>
                            <div class="research-text-content">
                                <h3 class="research-subtitle">Abstract</h3>
                                <p class="research-description">
                                    Comprehensive review and analysis of wearable devices for spinal posture monitoring. 
                                    This research evaluates current technologies, their effectiveness, and potential 
                                    improvements for postural health.
                                </p>
                                <h3 class="research-subtitle">Scope</h3>
                                <p class="research-description">
                                    Analysis of existing wearable devices, evaluation of monitoring accuracy, 
                                    and recommendations for future development in spinal posture tracking technology.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content-section" id="photos-section" style="display: none;">
                    <div class="section-content-wrapper">
                        <h2 class="section-main-title">Photos</h2>
                        <div class="photos-content">
                            <div class="photo-grid">
                                <div class="photo-item photo-item-1">
                                    <img src="images/IMG_8492.jpeg" alt="Photo 1" class="photo-image">
                                    <div class="photo-caption">[25.Nov] I attended the BlendED AI+X meetup</div>
                                </div>
                                <div class="photo-item photo-item-2">
                                    <img src="images/photo2.jpg" alt="Photo 2" class="photo-image">
                                    <div class="photo-caption">Caption 2</div>
                                </div>
                                <div class="photo-item photo-item-3">
                                    <img src="images/photo3.jpg" alt="Photo 3" class="photo-image">
                                    <div class="photo-caption">Caption 3</div>
                                </div>
                                <div class="photo-item photo-item-4">
                                    <img src="images/photo4.jpg" alt="Photo 4" class="photo-image">
                                    <div class="photo-caption">Caption 4</div>
                                </div>
                                <div class="photo-item photo-item-5">
                                    <img src="images/photo5.jpg" alt="Photo 5" class="photo-image">
                                    <div class="photo-caption">Caption 5</div>
                                </div>
                                <div class="photo-item photo-item-6">
                                    <img src="images/photo6.jpg" alt="Photo 6" class="photo-image">
                                    <div class="photo-caption">Caption 6</div>
                                </div>
                                <div class="photo-item photo-item-7">
                                    <img src="images/photo7.jpg" alt="Photo 7" class="photo-image">
                                    <div class="photo-caption">Caption 7</div>
                                </div>
                                <div class="photo-item photo-item-8">
                                    <img src="images/photo8.jpg" alt="Photo 8" class="photo-image">
                                    <div class="photo-caption">Caption 8</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <script src="script.js"></script>
</body>
</html>

